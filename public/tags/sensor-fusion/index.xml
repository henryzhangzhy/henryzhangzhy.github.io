<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Sensor Fusion on Hengyuan Zhang</title>
        <link>http://localhost:1313/tags/sensor-fusion/</link>
        <description>Recent content in Sensor Fusion on Hengyuan Zhang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Hengyuan Zhang</copyright>
        <lastBuildDate>Mon, 23 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/sensor-fusion/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Probabilistic Semantic Mapping for Autonomous Driving Applications</title>
        <link>http://localhost:1313/p/probabilistic-semantic-mapping-for-autonomous-driving-applications/</link>
        <pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/probabilistic-semantic-mapping-for-autonomous-driving-applications/</guid>
        <description>&lt;img src="http://localhost:1313/p/probabilistic-semantic-mapping-for-autonomous-driving-applications/semantic_map_2020_zoom.png" alt="Featured image of post Probabilistic Semantic Mapping for Autonomous Driving Applications" /&gt;&lt;p&gt;This is a work we submitted to IROS2020, co-authored by David Paz, Qinru Li, and Hao Xiang. We applied state-of-the-art semantic segmentation network to build a local map with semantic information such as roads, lanes and crosswalks based on vision and lidar data.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Autonomous driving today still strongly rely on high-definition maps, which often encodes the road, lane, traffic light, crosswalk information and so forth. High-definition map often requires large amount of human labeling. Part of the time-consuming step is to extract the semantic information [1]. Thus our work aims at moving a step closer to HD map automation by automatic semantic annotation.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;semantic-segmentation&#34;&gt;Semantic Segmentation&lt;/h3&gt;
&lt;p&gt;A network trained with DeepLabV3Plus [2] architecture on the Mapillary Vistas dataset [3]. The network trained with publicly available dataset achieves great generalization. We directly applied it to our scenario without fine-tuning.&lt;/p&gt;
&lt;h3 id=&#34;semantic-association&#34;&gt;Semantic Association&lt;/h3&gt;
&lt;p&gt;How to associate the semantic labels in 2D image with the map? Methods assuming a flat road and fixed camera are subject to nosie. Since our vehicle is localized with respect to the local point cloud map, we project a segment of the map into the image and retrieve the semantic labels.&lt;/p&gt;
&lt;h3 id=&#34;semantic-mapping&#34;&gt;Semantic Mapping&lt;/h3&gt;
&lt;p&gt;The local point cloud with semnatic label is good at visualization, but not for information retrieval. Also, it is hard to maintain temporal consistency on the point cloud map. Instead, we use a probabilistic map to reprensent the local information. for each cell, we encode the log odds of the cell being each class. For each semantic point cloud, we project the semantic label on the map cell and update the corresponding log odds.&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;See &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.04894&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;observations&#34;&gt;Observations&lt;/h2&gt;
&lt;p&gt;Semantic segmentation network trained on publicly available dataset generalized well. It performs better for close range. For this architecture on our image, the speed is about 4fps. It is the bottle neck of the entire system. Mapping runs super fast, often within 0.05s.
Probabilistic map is very robust to noise. Although semantic segmentation make mistakes when the objects are far away, it will correct it overtime.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] J. Jiao. Machine learning assisted high-definition map creation. In 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC), volume 01, pages 367–373, July 2018.&lt;/p&gt;
&lt;p&gt;[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. Lecture Notes in Computer Science, page 833–851, 2018.&lt;/p&gt;
&lt;p&gt;[3] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulò, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>A Multi Sensor Fusion System for Object Detection and Tracking</title>
        <link>http://localhost:1313/p/a-multi-sensor-fusion-system-for-object-detection-and-tracking/</link>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/a-multi-sensor-fusion-system-for-object-detection-and-tracking/</guid>
        <description>&lt;p&gt;A paper on sensor fusion system using EKF to fuse LIDARs, raders and cameras.&lt;/p&gt;
&lt;p&gt;This paper presents a two-layer fusion system. A sensor layer processes the raw sensor data and generates features and then generates proposals based on the features, partly across multiple sensors. The fusion layer fuses the sensor measurements using an Extended Kalman Fileter (EKF). Where measurements from all sensors are processed sequentially and asynchronously. Measurements are associated with the tracked objects based on their type. Each type of measuremnt has its own measurement model for update the associated object.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sensor-setup&#34;&gt;Sensor Setup&lt;/h2&gt;
&lt;p&gt;The platform introduced in this work use 6 LIDAR and radar pairs and three cameras for perception.&lt;/p&gt;
&lt;p&gt;LIDARs are 4-layer LIDAR with range up to 200m.
Radar1 has a max range of 250m and radar2 can be configured to 60m or 175m.
Two cameras, one in the front, one in the back have a resolution of 640*480.
The third camera is a thermal camera.&lt;/p&gt;
&lt;p&gt;Any object within 200 meters will be projected onto sensor converge and within 60 meters will be seen by at least two different types of sensors.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sensor-layer&#34;&gt;Sensor Layer&lt;/h2&gt;
&lt;p&gt;The sensor layer is made of sensor-specific modules. Each module takes the raw data and uses sensor-specific algorithms to generate high level features. And object proposals will be generated from these features.&lt;/p&gt;
&lt;p&gt;For radars, we can get 2D position and direct measuremnt of the speed of the target. The measurement at time $k$ would be represented as&lt;/p&gt;
&lt;p&gt;$z^R(k) = {r_1, r_2, &amp;hellip;, r_p},$&lt;/p&gt;
&lt;p&gt;where $r_i = [x, y, dx, dy]^\top, i = 1, 2, &amp;hellip;, p.$&lt;/p&gt;
&lt;p&gt;All six LIDARs are treated as one homogeneous sensor. Analyzing their measurements using built-in segmentation and extract features line segments or junctions of lines (&amp;ldquo;L&amp;rdquo;) shape [2]. we can get pose in 3D and size (partially)&lt;/p&gt;
&lt;p&gt;$z^L(k) = {l_1, l_2, &amp;hellip;, l_q},$&lt;/p&gt;
&lt;p&gt;where $l_i = [x, y, \theta, dx, dy, w, l]^\top, i = 1, 2, &amp;hellip;, q.$&lt;/p&gt;
&lt;p&gt;For cameras, we can get the bounding box in the image frame and a class label.&lt;/p&gt;
&lt;p&gt;$z^C(k) = {c_1, c_2, &amp;hellip;, c_r},$&lt;/p&gt;
&lt;p&gt;where $c_i = [x_1, y_1, x_2, y_2, class]^\top, i = 1, 2, &amp;hellip;, r$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fusion-layer&#34;&gt;Fusion Layer&lt;/h2&gt;
&lt;p&gt;The fusion of multiple sensor measurements employ the sequential-sensor method. Which treats observations from individual sensors idnependently and sequentially feeds them to the EKF&amp;rsquo;s estimation process.&lt;/p&gt;
&lt;h3 id=&#34;tracking-models&#34;&gt;Tracking Models&lt;/h3&gt;
&lt;h4 id=&#34;motion-models&#34;&gt;Motion Models&lt;/h4&gt;
&lt;p&gt;A point model and a 3D box model is used for motion prediction. A point model is effective in localizing objects that are far away and we don&amp;rsquo;t care about its shape and orientation. A 3D model is effective for objects that are near the vehicle and provides oritentaion and shape estimation.&lt;/p&gt;
&lt;p&gt;Here the point model is a constant acceleration model in 2D:&lt;/p&gt;
&lt;p&gt;$x(k) = [x, y ,v_x ,v_y, a_x, a_y]^\top$.&lt;/p&gt;
&lt;p&gt;and the box model is represented by:&lt;/p&gt;
&lt;p&gt;$x(k) = [x, y, \theta, v, \omega, a, w, l, h]^\top$.&lt;/p&gt;
&lt;p&gt;where $\theta$ is yaw angle, $\omega$ is yaw rate, $w$ is width, $l$ is length, $h$ is height.&lt;/p&gt;
&lt;p&gt;For more on motion model, [4] is a previous work on this.&lt;/p&gt;
&lt;h4 id=&#34;observation-models&#34;&gt;Observation Models&lt;/h4&gt;
&lt;p&gt;Observations from sensors will be associated with corresponding objects, then observation model is applied to update the state estimation. Data association will be introduced in the next section. If an observation is not associated with any esisting object, object instantiation should happen.&lt;/p&gt;
&lt;p&gt;We have observations from three sensors, they provide different information about the model. And we have both point model and box model. If we have corresponding update for all possibilities, we will have six observation models.&lt;/p&gt;
&lt;p&gt;But notice that radar cannot update the box target, it only update the point target.&lt;/p&gt;
&lt;p&gt;LIDAR measurements are primarily used for update the box target, it also can bu used to update the point target.&lt;/p&gt;
&lt;p&gt;Camera measurements cannot be used for position estimation due to poor depth estimation. Thus it is only applied to the box model for update the width and height.&lt;/p&gt;
&lt;p&gt;Details about the update step are not list in this post currently.&lt;/p&gt;
&lt;h3 id=&#34;data-association&#34;&gt;Data Association&lt;/h3&gt;
&lt;p&gt;Measurement data is sequentially sent to the EKF, thus the association process will handle one measurement to all objects. My interpretation is that a prediction step for all objects should happen before association. The association of prediction and measurement is sensor dependent.&lt;/p&gt;
&lt;p&gt;For LIDAR meansurements, association of point model is based on distance of prediction and measurment. For box models, possible interpretations of the edge target is generated and interpretations with max overlap with predicted object is selected. This in pratice occasionally fails thus vision target is utilized here. When vision target give higher reponse on a particular view of a vehicle (eg. back view), we can filter interpretations of other views (eg. side view).&lt;/p&gt;
&lt;p&gt;Association of camera measurements is achieved by projecting the prediction of the point model or box model into the pinhole camera model and compute the distance to the midpoint of the bottom line of the detected bounding box. The object whose point is the nearest neighbbor is associated.&lt;/p&gt;
&lt;p&gt;For association of radar measurements, a set of possible point targets are generated from predected model. Specifically, several points along the contour is generated for a box model and one point for a point model. The enhancement for a box model using multiple points is due to poor lateral position estimation from radar. Association is made by the nearest-neighbor approach.&lt;/p&gt;
&lt;h3 id=&#34;movement-classifications&#34;&gt;Movement Classifications&lt;/h3&gt;
&lt;p&gt;Movement Classification is hard for LIDAR detection cause shape changes when the car is driving and the center is hard to estimate. I had a headache with this when I was doing shape estimation. This method is definitely worth trying.&lt;/p&gt;
&lt;p&gt;Two flags are set for movement classification same as [4].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;movement history flag&lt;/strong&gt; is raised when the tracking system determines that the position of a tracked object has significantly changed. The distance traveled is computed from the last time stamp that the object has been classified as not observed moving. Emprically determined thresholds for pedestrians, vehicles and cyclists are used to compare with the distance.&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;movement state flag&lt;/strong&gt; is raised when the tracking system determines that the object is moving. Radars provide the directly estimation of moving state while LIDAR can estimate the speed over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;
&lt;p&gt;Result shown 93.7% by human inspection with 5.7 false positives per minute. Not sure what this exactly means.&lt;/p&gt;
&lt;p&gt;I think this is a very clear paper about sensor fusion. I like the proposed system.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;[1]  Cho, Hyunggi, et al. A multi-sensor fusion system for moving object detection and tracking in urban driving environments. &lt;em&gt;Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE&lt;/em&gt;, 2014.&lt;/p&gt;
&lt;p&gt;[2] C. Mertz et al. Moving Object Detection with Laser Scanners. &lt;em&gt;Journal of Field Robotics&lt;/em&gt;, 30(1):17-43, 2013.&lt;/p&gt;
&lt;p&gt;[3] H. Durrant-Whyte. &lt;em&gt;Multi Sensor Data Fusion&lt;/em&gt;. Lecture Notes, 2006.&lt;/p&gt;
&lt;p&gt;[4] M. S. Darms, P. Rybski, C. Baker, and C. Urmson. Obstacle detection and tracking for the urban challenge. &lt;em&gt;IEEE Transaction on Intelligent Transportation Systems&lt;/em&gt;, 10(3), 2009.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Sensor Fusion for Autonomous Vehicles</title>
        <link>http://localhost:1313/p/sensor-fusion-for-autonomous-vehicles/</link>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/sensor-fusion-for-autonomous-vehicles/</guid>
        <description>&lt;p&gt;Explore sensor fusion methods. Index page of findings in this area.&lt;/p&gt;
&lt;p&gt;Sensor fusion is to extract the information from heterogeneous sensors that complement each other. For the sensors that are commonly used in Autonomous Vehicles, like LIDARs, radars and cameras, this is obvious.&lt;/p&gt;
&lt;p&gt;Cameras are rich in texture and semantic information, like color, material, and signs. But they are poor at depth estimation. LIDARs can provide accurate depth information, using 3d point clouds, but everything is just a point cloud with a 3D position and a reflection rate. You cannnot distinguish a red light and a green light. radars can directly provide speed estimation, which is usually better than estimating using LIDARs[1].&lt;/p&gt;
&lt;p&gt;Recent advances in Autonomous Vehicles are highly dependent on sensor fusion. And companies like Waymo, Lyft, Zoox are using multi-sensor settings on their car. While research in single modal perception has a lot of advances, like deep-learning based visual recognition, new representation for point cloud learning, the fusion of them is unclear to me. This area is not much introduced in class and I feel like I need to invest some time on figuring out the big picture in this area by reading literatures.&lt;/p&gt;
&lt;p&gt;I would divide the sensor fusion systems into several categories and talk about them. It includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mathematical modeling methods&lt;/li&gt;
&lt;li&gt;deep learning methods&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mathematical-modeling-methods&#34;&gt;Mathematical Modeling Methods&lt;/h2&gt;
&lt;h3 id=&#34;a-multi-sensor-fusion-system-for-moving-object-detection-and-tracking-in-urban-driving-environmentshttpshenryzhangzhygithubio20190812a-multi-sensor-fusion-system-for-moving-object-detection-and-tracking-in-urban-driving-environmentshtml&#34;&gt;&lt;a class=&#34;link&#34; href=&#34;https://henryzhangzhy.github.io/2019/08/12/a-multi-sensor-fusion-system-for-moving-object-detection-and-tracking-in-urban-driving-environments.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Multi-Sensor Fusion System for Moving Object Detection and Tracking in Urban Driving Environments&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This paper presents a two-layer fusion system. A sensor layer processes the raw sensor data and generates features and then generates proposals based on the features, partly across multiple sensors. The fusion layer fuses the sensor measurements using an Extended Kalman Fileter (EKF). Where measurements from all sensors are processed sequentially and asynchronously. Measurements are associated with the tracked objects based on their type. Each type of measuremnt has its own measurement model for update the associated object.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;deep-learning-methods&#34;&gt;Deep Learning methods&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;[1]  Cho, Hyunggi, et al. ”A multi-sensor fusion system for moving object detection and tracking in urban driving environments.” Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, 2014.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Sensors for Autonomous Vehicles</title>
        <link>http://localhost:1313/p/sensors-for-autonomous-vehicles/</link>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/sensors-for-autonomous-vehicles/</guid>
        <description>&lt;p&gt;Reference page of sensor characteristics.&lt;/p&gt;
&lt;p&gt;Inevitably, we need to talk about sensor characteristics when discussing about sensor fusion. I don&amp;rsquo;t want to repeat them too much when I am writing posts about sensor fusion. And it would be interesting to see how people think of sensors generally. So all discussion about sensors will go here.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sensor-characteristics&#34;&gt;Sensor Characteristics&lt;/h2&gt;
&lt;h3 id=&#34;lidar&#34;&gt;LIDAR&lt;/h3&gt;
&lt;h3 id=&#34;camera&#34;&gt;Camera&lt;/h3&gt;
&lt;h3 id=&#34;radar&#34;&gt;Radar&lt;/h3&gt;
&lt;h3 id=&#34;ultra-sonic&#34;&gt;ultra-sonic&lt;/h3&gt;
&lt;h3 id=&#34;kinect&#34;&gt;kinect&lt;/h3&gt;
&lt;p&gt;For Autonomous Vehicle? I am kidding.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;sensor-comparison&#34;&gt;Sensor comparison&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>Multi-sensor Data Fusion Hugh Durrant Whyte</title>
        <link>http://localhost:1313/p/multi-sensor-data-fusion-hugh-durrant-whyte/</link>
        <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/multi-sensor-data-fusion-hugh-durrant-whyte/</guid>
        <description>&lt;p&gt;Notes on &lt;em&gt;Multi Sensor Data Fusion&lt;/em&gt; from Hugh Durrant Whyte.&lt;/p&gt;
&lt;p&gt;This is a lecture note referenced in the paper that I talked about in the post &lt;a class=&#34;link&#34; href=&#34;https://henryzhangzhy.github.io/2019/08/12/a-multi-sensor-fusion-system-for-moving-object-detection-and-tracking-in-urban-driving-environments.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;a multi-sensor fusion system for moving object detection and tracking in urban driving environments&lt;/a&gt;. In this lecture note, the probability foundations and some methods of multi sensor fusion are discussed.&lt;/p&gt;
&lt;h2 id=&#34;2-probabilistic-data-fusion&#34;&gt;2 Probabilistic Data Fusion&lt;/h2&gt;
&lt;h3 id=&#34;21-probabilistic-models&#34;&gt;2.1 Probabilistic Models&lt;/h3&gt;
&lt;p&gt;Introduced definition of probability density function and its property. Conditional Probability and total probability theorem were explained. Details goes my post about &lt;a class=&#34;link&#34; href=&#34;https://henryzhangzhy.github.io/2019/08/15/probability-notes.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;probability notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We use random variable $\mathbf{x}$ to represent the state that we want to estimate. use random variable $\mathbf{z}$ to represent the observation that we get.&lt;/p&gt;
&lt;h3 id=&#34;22-probabilistic-methods&#34;&gt;2.2 Probabilistic Methods&lt;/h3&gt;
&lt;h4 id=&#34;221-bayes-theorem&#34;&gt;2.2.1 Bayes theorem&lt;/h4&gt;
&lt;p&gt;Bayes theorem offers the tool to directly combine observations and prior beliefs.
$$\begin{equation}
P(\mathbf{x} | \mathbf{z}) = \frac{P(\mathbf{z} | \mathbf{x}) P(\mathbf{x})}{P(\mathbf{z})} .
\end{equation}$$&lt;/p&gt;
&lt;h4 id=&#34;222-data-fusion-using-bayes-theorem&#34;&gt;2.2.2 Data Fusion using Bayes Theorem&lt;/h4&gt;
&lt;p&gt;For multi sensor fusion, we want to combine the information of multiple observations, thus we have
$$\begin{equation}
P(\mathbf{x} | \mathbf{Z^n}) = \frac{P(\mathbf{Z^n} | \mathbf{x}) P(\mathbf{x})}{P(\mathbf{Z^n})} .
\end{equation}$$
Since it is often the case that observations are only dependent on the internal state $\mathbf{x}$, or conditionally independent on $\mathbf{x}$. We have
$$\begin{equation}
P(\mathbf{x} | \mathbf{Z^n}) = CP(\mathbf{x}) \sum_{i=1}^{n}P(\mathbf{z_i} | \mathbf{x}).
\end{equation}$$&lt;/p&gt;
&lt;h4 id=&#34;223-recursive-bayes-updating&#34;&gt;2.2.3 Recursive Bayes Updating&lt;/h4&gt;
&lt;p&gt;Do we have to start from the initial prior all the time, multiplying all the probability of observations? We can actually do it recursively.&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
P(\mathbf{x}|\mathbf{Z^k}) = \frac{P(\mathbf{Z^k}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{Z^k})}
\end{equation}$$
$$\begin{equation}
= \frac{P(\mathbf{z_k}, \mathbf{Z^{k-1}}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{Z^k})}
\end{equation}$$
$$\begin{equation}
= \frac{P(\mathbf{z_k} | \mathbf{x})P(\mathbf{Z^{k-1}}|\mathbf{x})P(\mathbf{x})}{P(\mathbf{Z^k})}
\end{equation}$$
$$\begin{equation}
= \frac{P(\mathbf{z_k} | \mathbf{x})P(\mathbf{x}|\mathbf{Z^{k-1}})P(\mathbf{Z^{k-1}})}{P(\mathbf{Z^k})}
\end{equation}$$
$$\begin{equation}
= \frac{P(\mathbf{z_k} | \mathbf{x})P(\mathbf{x}|\mathbf{Z^{k-1}})}{P(\mathbf{z_k}|\mathbf{Z^{k-1}})}.&lt;br&gt;
\end{equation}$$&lt;/p&gt;
&lt;h4 id=&#34;224-data-dependency-and-bayes-networks&#34;&gt;2.2.4 Data Dependency and Bayes Networks&lt;/h4&gt;
&lt;p&gt;Short Introduction to be belief networks.&lt;/p&gt;
&lt;h4 id=&#34;225-distributed-data-fusion-with-bayes-theorem&#34;&gt;2.2.5 Distributed Data Fusion with Bayes Theorem&lt;/h4&gt;
&lt;p&gt;Distributed means raw data is processed in the sensor module. This has the advantage that each sensor is &lt;strong&gt;&amp;lsquo;modular&amp;rsquo;&lt;/strong&gt; and talks in a common &lt;strong&gt;&amp;lsquo;state&amp;rsquo;&lt;/strong&gt; language. However, it has the disadvantage that a complete likelihood, rather than a single observation, must be communicated.&lt;/p&gt;
&lt;h4 id=&#34;226-data-fusion-with-log-likelihoods&#34;&gt;2.2.6 Data Fusion with Log-Likelihoods&lt;/h4&gt;
&lt;p&gt;If we take log of the Beyes Theorem, it becomes
$$\begin{equation}
\mathbf{l}(\mathbf{x} | \mathbf{z}) = \mathbf{l}(\mathbf{z} | \mathbf{x}) + \mathbf{l}(\mathbf{x}) - \mathbf{l}(\mathbf{z}).
\end{equation}$$&lt;/p&gt;
&lt;p&gt;This simplifies the calculation.&lt;/p&gt;
&lt;h3 id=&#34;23-information-measures&#34;&gt;2.3 Information Measures&lt;/h3&gt;
&lt;p&gt;Information is a measure of the compactness of a distribution. The shannon information (or entropy) and the Fisher information are two probabilistic measures of information of particular value in data fusion problems.&lt;/p&gt;
&lt;h4 id=&#34;231-entropic-information&#34;&gt;2.3.1 Entropic Information&lt;/h4&gt;
&lt;p&gt;$$\begin{equation}
H_P(\mathbf{x}) \triangleq - \mathop{\mathbb{E}} \lbrace \log P(\mathbf{x})\rbrace = - \int_{-\infty}^{\infty}P(\mathbf{x}) \mathrm{d}\mathbf{x}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Note that $H_P(\cdot)$ is not strictly a function of $\mathbf{x}$ but is rather a function of the distribution $P(\cdot)$.&lt;/p&gt;
&lt;h4 id=&#34;232-conditional-entropy&#34;&gt;2.3.2 Conditional Entropy&lt;/h4&gt;
&lt;p&gt;$$\begin{equation}
H_P(\mathbf{x} | z_j) \triangleq - \mathop{\mathbb{E}} \lbrace \log P(\mathbf{x} | z_j) \rbrace = - \int_{-\infty}^{\infty}P(\mathbf{x} | z_j) \mathrm{d}\mathbf{x}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
\bar{H}(\mathbf{x} | \mathbf{z}) \triangleq - \mathop{\mathbb{E}} \lbrace H(\mathbf{x} | \mathbf{z}) \rbrace = - \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} P(\mathbf{x}, \mathbf{z}) \log P(\mathbf{x} | \mathbf{z}) \mathrm{d} \mathbf{x} \mathrm{d}\mathbf{z}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;It shows the entropy of an n dimensional Gaussian Distribution is
$$\begin{equation}
- \frac{1}{2} \log \lbrack (2 \pi e)^n | \mathbf{P} | \rbrack.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;The entropy is proportional to the log of the determinant of the covariance. The determinant of a matrix is a volume measure (recall that the determinant is the product of the eigenvalues of a matrix and the eigenvalues define axis lengths in n space).&lt;/p&gt;
&lt;h4 id=&#34;233-mutual-information&#34;&gt;2.3.3 Mutual Information&lt;/h4&gt;
&lt;p&gt;$$\begin{equation}
I(\mathbf{x}, \mathbf{z}) \triangleq - \mathop{\mathbb{E}} \lbrace \log \frac{P(\mathbf{x}, \mathbf{z})}{P(\mathbf{x}) P(\mathbf{z})} \rbrace.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
I(\mathbf{x}, \mathbf{z}) = H(\mathbf{x}) + H(\mathbf{z}) - H(\mathbf{x}, \mathbf{z}).
\end{equation}$$&lt;/p&gt;
&lt;p&gt;The process of predicting information gain, making a decision on sensing action, then sensing, is an efficient and effective way of managing sensing resources and of determining optimal sensing policies.&lt;/p&gt;
&lt;h4 id=&#34;234-fisher-information&#34;&gt;2.3.4 Fisher Information&lt;/h4&gt;
&lt;p&gt;The Fisher information $J(\mathbf{x})$ is defined as the second derivative of the log-likelihood
$$\begin{equation} J(\mathbf{x}) \triangleq \frac{\mathrm{d^2}}{\mathrm{d}\mathbf{x^2}} \log P(\mathbf{x}).
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Generally in a matrix form while cross entropy is a scaler.&lt;/p&gt;
&lt;h3 id=&#34;24-alternatives-to-probability&#34;&gt;2.4 Alternatives to Probability&lt;/h3&gt;
&lt;p&gt;Interval Calculus, Fuzzy logic and Evidential Reasoning is introduced shortly.&lt;/p&gt;
&lt;h2 id=&#34;3-multi-sensor-estimation&#34;&gt;3 Multi-Sensor Estimation&lt;/h2&gt;
&lt;h3 id=&#34;31-the-kalman-filter&#34;&gt;3.1 The Kalman Filter&lt;/h3&gt;
&lt;p&gt;With certain assuptions about the observation and process models, the resulting estimate $\dot{\mathbf{x}}(t)$ minimises mean-squared error
$$\begin{equation}
L(t) = \int_{-\infty}^{\infty} (\mathbf{x}(t) - \hat{\mathbf{x}}(t))^T (\mathbf{x}(t) - \hat{\mathbf{x}}(t)) P(\mathbf{x}(t) | \mathbf{Z}^t) \mathrm{d} \mathbf{x}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Differentiation of the above equation with respect to $\mathbf{x}(t)$ and setting equal to zero gives
$$\begin{equation}
\hat{\mathbf{x}}(t) = \int_{-\infty}^{\infty} \mathbf{x}(t) P(\mathbf{x}(t) | \mathbf{Z}^t) \mathrm{d} \mathbf{x}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;which is simply the conditional mean $ \hat{\mathbf{x}}(t) = \mathop{\mathbb{E}}(\mathbf{x}(t) | \mathbf{Z}^t).$&lt;/p&gt;
&lt;p&gt;The Kalman Filter, and indeed any mean-squared-error estimator, computes an estimate which is the conditional mean; an average, rather than a most likely value. (Q: what is the most likely value ?)&lt;/p&gt;
&lt;h4 id=&#34;311-state-and-sensor-models&#34;&gt;3.1.1 State and Sensor Models&lt;/h4&gt;
&lt;p&gt;This section defineds the notations for Kalman Filter. And shows how the model is discretized.&lt;/p&gt;
&lt;p&gt;The author claimed that it is always best to start with a continuous-time model for the state and then construct a discrete model, rather than stating the discrete model at the outset. This allows for correct identification of noise transfers and noise correlations. ??&lt;/p&gt;
&lt;h4 id=&#34;312-the-kalman-filter-algorithm&#34;&gt;3.1.2 The Kalman Filter Algorithm&lt;/h4&gt;
&lt;p&gt;Prediction Step:
$$\begin{equation}
\hat{\mathbf{x}}(k | k-1) = \mathbf{F}(k) \hat{\mathbf{x}}(k-1 | k-1) + \mathbf{B}(k) \mathbf{u}(k)
\end{equation}$$
$$\begin{equation}
\mathbf{P}(k | k-1) = \mathbf{F}(k) \mathbf{P}(k-1 | k-1) \mathbf{F}^T(k) + \mathbf{G}(k) \mathbf{Q}(k) \mathbf{G}^T(k)
\end{equation}$$&lt;/p&gt;
&lt;p&gt;Update Step:
$$\begin{equation}
v(k) = \mathbf{z}(k) - \mathbf{H}(k) \hat{\mathbf{x}}(k | k-1)
\end{equation}$$
$$\begin{equation}
\mathbf{S}(k) = \mathbf{R}(k) + \mathbf{H}(k) \mathbf{P}(k | k-1) \mathbf{H}^T(k)
\end{equation}$$
$$\begin{equation}
\hat{\mathbf{x}}(k | k) = \hat{\mathbf{x}}(k | k-1) - \mathbf{W}(k) v(k)
\end{equation}$$
$$\begin{equation}
\mathbf{P}(k | k) = \mathbf{P}(k | k-1) - \mathbf{W}(k) \mathbf{S}(k) \mathbf{W}^T(k)
\end{equation}$$
$$\begin{equation}
\mathbf{W}(k) = \mathbf{P}(k | k-1) \mathbf{H}^T(k) \mathbf{S}^{-1}(k)
\end{equation}$$&lt;/p&gt;
&lt;h4 id=&#34;313-the-innovation&#34;&gt;3.1.3 The Innovation&lt;/h4&gt;
&lt;p&gt;Because the &amp;rsquo;true&amp;rsquo; states are not usually available for comparison with the estimated states, the innovation is often the only measure of how well the estimator is performing. The most important property of innovation is that they form an orthogonal uncorrelated, white sequence,
$$\begin{equation}
\mathop{\mathbb{E}} \lbrace v(k) | \mathbf{Z}^{k-1} \rbrace = 0, \mathop{\mathbb{E}} \lbrace v(i)v^T(jj) \rbrace = \mathbf{S}(i) \delta_{ij}.
\end{equation}$$&lt;/p&gt;
&lt;p&gt;This can be exploited in monitoring filter performance.&lt;/p&gt;
&lt;h4 id=&#34;314-understanding-the-kalman-filter&#34;&gt;3.1.4 Understanding the Kalman Filter&lt;/h4&gt;
&lt;p&gt;There are three different, but related ways of thinking about the Kalman filter algorithm&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Weighted average&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clearly, this interpretation of the Kalman filter holds only for those states which can be written as a linear combination of the observation vector.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional mean&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Orthogonal decomposition&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;315-steady-state-filter&#34;&gt;3.1.5 Steady State Filter&lt;/h4&gt;
&lt;p&gt;Using the fact that if filtering is synchronous, and state and observation matrics time-invariant, the filter variance will converge quickly, thus $\mathbf{W}$ matrix is also converged, we can skip the computation and use paramatric model to estimate it, reduce the computation greatly.&lt;/p&gt;
&lt;h4 id=&#34;316-asynchronous-delayed-and-asequent-observations&#34;&gt;3.1.6 Asynchronous, Delayed and Asequent observations&lt;/h4&gt;
&lt;p&gt;Asynchronous means observations from different sensors don&amp;rsquo;t come at the same time, but without delay. Can be addressed by taking time into account, make a prediction at observing time.
Delayed means some observations is received by the filter with some delay after it is observed. Can be addressed by dropping the current prediction and make a prediction of the observing time of that sensor.
Asequent means delay from different sensor cause the order of receiving is different from the the order of observations. It cannot be addressed easily unless you keep some observations and recompute the estimation. The author claim this can possibly be solved by a &lt;strong&gt;inverse-covariance filter ??&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;317-the-extended-kalman-filter&#34;&gt;3.1.7 The Extended Kalman Filter&lt;/h4&gt;
&lt;p&gt;Linearization at the estimation point.&lt;/p&gt;
&lt;p&gt;A few comments from the author:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Jacobians needs to be recomputed every time.&lt;/li&gt;
&lt;li&gt;The model is based on lieanrization at the estimated trajectory, thus if the estimation is not accurate, the second or higher order term cannot be ignored and make it useless. This also requires that initialization should be careful.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;32-the-multi-sensor-kalman-filter&#34;&gt;3.2 The Multi-Sensor Kalman Filter&lt;/h3&gt;
&lt;h4 id=&#34;321-observation-models&#34;&gt;3.2.1 Observation Models&lt;/h4&gt;
&lt;h4 id=&#34;322-the-group-sensor-method&#34;&gt;3.2.2 The Group-Sensor Method&lt;/h4&gt;
&lt;p&gt;It basiclly stacks all the observations and observation models, treating them as one sensor. The Kalman Filter can be directly applied. The problems is that sensors are assumed to be synchronous and computational complexity of matrix inversion is $\mathbf{O}(n^3)$ as the number of sensors increases.&lt;/p&gt;
&lt;h4 id=&#34;323-the-sequential-sensor-method&#34;&gt;3.2.3 The Sequential-Sensor Method&lt;/h4&gt;
&lt;p&gt;Treat each observation and observation model independently, make a prediction and update each observation is made. The requires too much predicitons and updates when sensor number increases, though it is linear of the number of sensors. CMU paper &lt;a class=&#34;link&#34; href=&#34;https://henryzhangzhy.github.io/2019/08/12/a-multi-sensor-fusion-system-for-moving-object-detection-and-tracking-in-urban-driving-environments.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;a multi-sensor fusion system for moving object detection and tracking in urban driving environments&lt;/a&gt; applied this method.&lt;/p&gt;
&lt;h4 id=&#34;324-the-inverse-covariance-form&#34;&gt;3.2.4 The Inverse-Covariance Form&lt;/h4&gt;
&lt;p&gt;This is essentially a information filter, the good side is it simplifies the update step, while the only inversion would be the $P$ matrix, only of the size of states and not related to the number of sensors.&lt;/p&gt;
&lt;h4 id=&#34;325-track-to-track-fusion&#34;&gt;3.2.5 Track-to-Track Fusion&lt;/h4&gt;
&lt;p&gt;Sensor will do filtering locally and pass the posterior to the center, the center will fuse them together. Method can be covariance-weighted Average.&lt;/p&gt;
&lt;h3 id=&#34;33-non-linear-data-fusion-methods&#34;&gt;3.3 Non-linear Data Fusion Methods&lt;/h3&gt;
&lt;p&gt;Bayes filter is mentioned. Listed particle-filter and the sum-of-gaussians (gaussian mixture) method.&lt;/p&gt;
&lt;h3 id=&#34;34-multi-sensor-data-association&#34;&gt;3.4 Multi-Sensor Data Association&lt;/h3&gt;
&lt;p&gt;The idea is to use the nomalized innovation distance.&lt;/p&gt;
&lt;h4 id=&#34;341-the-nearest-neighbour-standard-filter&#34;&gt;3.4.1 The Nearest-Neighbour Standard Filter&lt;/h4&gt;
&lt;p&gt;Match the closest one, not suitable for highly clustered detections.&lt;/p&gt;
&lt;h4 id=&#34;342-the-probabilistic-data-association-filter&#34;&gt;3.4.2 The Probabilistic Data Association Filter&lt;/h4&gt;
&lt;p&gt;Keep a mixture of all the observations within a certain range of the prediction to form a combined state and do the prediction again.&lt;/p&gt;
&lt;p&gt;The good thing is you are never wrong, but you are never right as well.&lt;/p&gt;
&lt;h4 id=&#34;343-the-multiple-hypothesis-tracking-mht-filter&#34;&gt;3.4.3 The Multiple Hypothesis Tracking (MHT) Filter&lt;/h4&gt;
&lt;p&gt;Keep a hypothesis for all observations within certain range. Compute the likelihood and drop protential not good trackings.&lt;/p&gt;
&lt;h4 id=&#34;344-data-association-in-track-to-track-fusion&#34;&gt;3.4.4 Data Association in Track-to-Track Fusion&lt;/h4&gt;
&lt;p&gt;Weighted average.&lt;/p&gt;
&lt;h4 id=&#34;345-maintaining-and-managing-track-files&#34;&gt;3.4.5 Maintaining and Managing Track Files&lt;/h4&gt;
&lt;p&gt;Refer to other literature.&lt;/p&gt;
&lt;h2 id=&#34;others&#34;&gt;Others&lt;/h2&gt;
&lt;p&gt;Section 4 metions about distributed sensor fusion and 5 metions about decision make, which are not of interest here.&lt;/p&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;How do we discretize the kalman filter?&lt;/li&gt;
&lt;li&gt;How is $\mathbf{HPH}^T$ a projection?&lt;/li&gt;
&lt;li&gt;How is the gain being computed?&lt;/li&gt;
&lt;li&gt;What are the three interpretation?&lt;/li&gt;
&lt;li&gt;How to use information filter to solve asequent data?&lt;/li&gt;
&lt;li&gt;How to use information filter to solve multi sensor problems?&lt;/li&gt;
&lt;li&gt;How is the steady state filter parameter being choosed?&lt;/li&gt;
&lt;li&gt;What are the difference of the association methods?&lt;/li&gt;
&lt;li&gt;Why $ \mathbf{GQG}^T$ in kalman filter prediction?&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
