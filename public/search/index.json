[{"content":"This is a work we submitted to IROS2020, co-authored by David Paz, Qinru Li, and Hao Xiang. We applied state-of-the-art semantic segmentation network to build a local map with semantic information such as roads, lanes and crosswalks based on vision and lidar data.\nMotivation Autonomous driving today still strongly rely on high-definition maps, which often encodes the road, lane, traffic light, crosswalk information and so forth. High-definition map often requires large amount of human labeling. Part of the time-consuming step is to extract the semantic information [1]. Thus our work aims at moving a step closer to HD map automation by automatic semantic annotation.\nMethod Semantic Segmentation A network trained with DeepLabV3Plus [2] architecture on the Mapillary Vistas dataset [3]. The network trained with publicly available dataset achieves great generalization. We directly applied it to our scenario without fine-tuning.\nSemantic Association How to associate the semantic labels in 2D image with the map? Methods assuming a flat road and fixed camera are subject to nosie. Since our vehicle is localized with respect to the local point cloud map, we project a segment of the map into the image and retrieve the semantic labels.\nSemantic Mapping The local point cloud with semnatic label is good at visualization, but not for information retrieval. Also, it is hard to maintain temporal consistency on the point cloud map. Instead, we use a probabilistic map to reprensent the local information. for each cell, we encode the log odds of the cell being each class. For each semantic point cloud, we project the semantic label on the map cell and update the corresponding log odds.\nExperiments See arXiv\nObservations Semantic segmentation network trained on publicly available dataset generalized well. It performs better for close range. For this architecture on our image, the speed is about 4fps. It is the bottle neck of the entire system. Mapping runs super fast, often within 0.05s. Probabilistic map is very robust to noise. Although semantic segmentation make mistakes when the objects are far away, it will correct it overtime.\nReference [1] J. Jiao. Machine learning assisted high-definition map creation. In 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC), volume 01, pages 367–373, July 2018.\n[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. Lecture Notes in Computer Science, page 833–851, 2018.\n[3] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulò, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017.\n","date":"2020-03-23T00:00:00Z","image":"http://localhost:1313/p/probabilistic-semantic-mapping-for-autonomous-driving-applications/semantic_map_2020_zoom_hu4dd5aea826cb103c15d459d0637191ab_595055_120x120_fill_box_smart1_3.png","permalink":"http://localhost:1313/p/probabilistic-semantic-mapping-for-autonomous-driving-applications/","title":"Probabilistic Semantic Mapping for Autonomous Driving Applications"},{"content":"A paper on sensor fusion system using EKF to fuse LIDARs, raders and cameras.\nThis paper presents a two-layer fusion system. A sensor layer processes the raw sensor data and generates features and then generates proposals based on the features, partly across multiple sensors. The fusion layer fuses the sensor measurements using an Extended Kalman Fileter (EKF). Where measurements from all sensors are processed sequentially and asynchronously. Measurements are associated with the tracked objects based on their type. Each type of measuremnt has its own measurement model for update the associated object.\nSensor Setup The platform introduced in this work use 6 LIDAR and radar pairs and three cameras for perception.\nLIDARs are 4-layer LIDAR with range up to 200m. Radar1 has a max range of 250m and radar2 can be configured to 60m or 175m. Two cameras, one in the front, one in the back have a resolution of 640*480. The third camera is a thermal camera.\nAny object within 200 meters will be projected onto sensor converge and within 60 meters will be seen by at least two different types of sensors.\nSensor Layer The sensor layer is made of sensor-specific modules. Each module takes the raw data and uses sensor-specific algorithms to generate high level features. And object proposals will be generated from these features.\nFor radars, we can get 2D position and direct measuremnt of the speed of the target. The measurement at time $k$ would be represented as\n$z^R(k) = {r_1, r_2, \u0026hellip;, r_p},$\nwhere $r_i = [x, y, dx, dy]^\\top, i = 1, 2, \u0026hellip;, p.$\nAll six LIDARs are treated as one homogeneous sensor. Analyzing their measurements using built-in segmentation and extract features line segments or junctions of lines (\u0026ldquo;L\u0026rdquo;) shape [2]. we can get pose in 3D and size (partially)\n$z^L(k) = {l_1, l_2, \u0026hellip;, l_q},$\nwhere $l_i = [x, y, \\theta, dx, dy, w, l]^\\top, i = 1, 2, \u0026hellip;, q.$\nFor cameras, we can get the bounding box in the image frame and a class label.\n$z^C(k) = {c_1, c_2, \u0026hellip;, c_r},$\nwhere $c_i = [x_1, y_1, x_2, y_2, class]^\\top, i = 1, 2, \u0026hellip;, r$.\nFusion Layer The fusion of multiple sensor measurements employ the sequential-sensor method. Which treats observations from individual sensors idnependently and sequentially feeds them to the EKF\u0026rsquo;s estimation process.\nTracking Models Motion Models A point model and a 3D box model is used for motion prediction. A point model is effective in localizing objects that are far away and we don\u0026rsquo;t care about its shape and orientation. A 3D model is effective for objects that are near the vehicle and provides oritentaion and shape estimation.\nHere the point model is a constant acceleration model in 2D:\n$x(k) = [x, y ,v_x ,v_y, a_x, a_y]^\\top$.\nand the box model is represented by:\n$x(k) = [x, y, \\theta, v, \\omega, a, w, l, h]^\\top$.\nwhere $\\theta$ is yaw angle, $\\omega$ is yaw rate, $w$ is width, $l$ is length, $h$ is height.\nFor more on motion model, [4] is a previous work on this.\nObservation Models Observations from sensors will be associated with corresponding objects, then observation model is applied to update the state estimation. Data association will be introduced in the next section. If an observation is not associated with any esisting object, object instantiation should happen.\nWe have observations from three sensors, they provide different information about the model. And we have both point model and box model. If we have corresponding update for all possibilities, we will have six observation models.\nBut notice that radar cannot update the box target, it only update the point target.\nLIDAR measurements are primarily used for update the box target, it also can bu used to update the point target.\nCamera measurements cannot be used for position estimation due to poor depth estimation. Thus it is only applied to the box model for update the width and height.\nDetails about the update step are not list in this post currently.\nData Association Measurement data is sequentially sent to the EKF, thus the association process will handle one measurement to all objects. My interpretation is that a prediction step for all objects should happen before association. The association of prediction and measurement is sensor dependent.\nFor LIDAR meansurements, association of point model is based on distance of prediction and measurment. For box models, possible interpretations of the edge target is generated and interpretations with max overlap with predicted object is selected. This in pratice occasionally fails thus vision target is utilized here. When vision target give higher reponse on a particular view of a vehicle (eg. back view), we can filter interpretations of other views (eg. side view).\nAssociation of camera measurements is achieved by projecting the prediction of the point model or box model into the pinhole camera model and compute the distance to the midpoint of the bottom line of the detected bounding box. The object whose point is the nearest neighbbor is associated.\nFor association of radar measurements, a set of possible point targets are generated from predected model. Specifically, several points along the contour is generated for a box model and one point for a point model. The enhancement for a box model using multiple points is due to poor lateral position estimation from radar. Association is made by the nearest-neighbor approach.\nMovement Classifications Movement Classification is hard for LIDAR detection cause shape changes when the car is driving and the center is hard to estimate. I had a headache with this when I was doing shape estimation. This method is definitely worth trying.\nTwo flags are set for movement classification same as [4].\nthe movement history flag is raised when the tracking system determines that the position of a tracked object has significantly changed. The distance traveled is computed from the last time stamp that the object has been classified as not observed moving. Emprically determined thresholds for pedestrians, vehicles and cyclists are used to compare with the distance. the movement state flag is raised when the tracking system determines that the object is moving. Radars provide the directly estimation of moving state while LIDAR can estimate the speed over time. Comments Result shown 93.7% by human inspection with 5.7 false positives per minute. Not sure what this exactly means.\nI think this is a very clear paper about sensor fusion. I like the proposed system.\nReference [1] Cho, Hyunggi, et al. A multi-sensor fusion system for moving object detection and tracking in urban driving environments. Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, 2014.\n[2] C. Mertz et al. Moving Object Detection with Laser Scanners. Journal of Field Robotics, 30(1):17-43, 2013.\n[3] H. Durrant-Whyte. Multi Sensor Data Fusion. Lecture Notes, 2006.\n[4] M. S. Darms, P. Rybski, C. Baker, and C. Urmson. Obstacle detection and tracking for the urban challenge. IEEE Transaction on Intelligent Transportation Systems, 10(3), 2009.\n","date":"2019-08-12T00:00:00Z","permalink":"http://localhost:1313/p/a-multi-sensor-fusion-system-for-object-detection-and-tracking/","title":"A Multi Sensor Fusion System for Object Detection and Tracking"},{"content":"Explore sensor fusion methods. Index page of findings in this area.\nSensor fusion is to extract the information from heterogeneous sensors that complement each other. For the sensors that are commonly used in Autonomous Vehicles, like LIDARs, radars and cameras, this is obvious.\nCameras are rich in texture and semantic information, like color, material, and signs. But they are poor at depth estimation. LIDARs can provide accurate depth information, using 3d point clouds, but everything is just a point cloud with a 3D position and a reflection rate. You cannnot distinguish a red light and a green light. radars can directly provide speed estimation, which is usually better than estimating using LIDARs[1].\nRecent advances in Autonomous Vehicles are highly dependent on sensor fusion. And companies like Waymo, Lyft, Zoox are using multi-sensor settings on their car. While research in single modal perception has a lot of advances, like deep-learning based visual recognition, new representation for point cloud learning, the fusion of them is unclear to me. This area is not much introduced in class and I feel like I need to invest some time on figuring out the big picture in this area by reading literatures.\nI would divide the sensor fusion systems into several categories and talk about them. It includes:\nmathematical modeling methods deep learning methods Mathematical Modeling Methods A Multi-Sensor Fusion System for Moving Object Detection and Tracking in Urban Driving Environments This paper presents a two-layer fusion system. A sensor layer processes the raw sensor data and generates features and then generates proposals based on the features, partly across multiple sensors. The fusion layer fuses the sensor measurements using an Extended Kalman Fileter (EKF). Where measurements from all sensors are processed sequentially and asynchronously. Measurements are associated with the tracked objects based on their type. Each type of measuremnt has its own measurement model for update the associated object.\nDeep Learning methods Reference [1] Cho, Hyunggi, et al. ”A multi-sensor fusion system for moving object detection and tracking in urban driving environments.” Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, 2014.\n","date":"2019-08-12T00:00:00Z","permalink":"http://localhost:1313/p/sensor-fusion-for-autonomous-vehicles/","title":"Sensor Fusion for Autonomous Vehicles"},{"content":"Reference page of sensor characteristics.\nInevitably, we need to talk about sensor characteristics when discussing about sensor fusion. I don\u0026rsquo;t want to repeat them too much when I am writing posts about sensor fusion. And it would be interesting to see how people think of sensors generally. So all discussion about sensors will go here.\nSensor Characteristics LIDAR Camera Radar ultra-sonic kinect For Autonomous Vehicle? I am kidding.\nSensor comparison ","date":"2019-08-12T00:00:00Z","permalink":"http://localhost:1313/p/sensors-for-autonomous-vehicles/","title":"Sensors for Autonomous Vehicles"},{"content":"A reference for markdown language.\nstructure\ntitle # second level title ## list a \u0026lsquo;-\u0026rsquo; followed by a space a \u0026lsquo;-\u0026rsquo; another one numbered list\nactual number doesn\u0026rsquo;t matter, just a number\nproperly indented paragraph, with blank line and space.\nanother sublist here * works for unordered lists + works the same - as well checked box\nunchecked checked format emphasis: _emphasis_ or *emphasis* strong emphasis: __strong emphasis__ or **strong emphasis** combined emphasis: **combined _emphasis_** strike through: ~~strike through~~ a new line in editor will result in no line break in the markdown display. insert an empty to get a new paragraph add \u0026lt;br/\u0026gt; to the end of the line to get a new line. a single back slash \\ works for markdown but not generated website and double backslashes \\\\ works for a website but not markdown. Double space works for both but is not recommanded as editors sometimes delete them while saving. code `inline code` inline code\n```python\ncode block\n```\n1 2 3 4 square = [x*2 for x in range(10)] square = [] for x in range(10): square.append(x**2) // indent requires four spaces. links Google: [Google](https://www.google.com)\nGoogle: [Google](https://www.google.com \u0026quot;Google Homepage\u0026quot;)\nreference style: [reference style][reference]\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\n[reference]: https://www.reddit.com\nInline-style: Reference-style: Github images\n1 ![Single Tracking](/assets/images/single_object_tracking_1.png) The format is ! [Alt Text] (link to image)\n![Single_Tracking]({{ site.url }}/assets/images/single_object_tracking_1.png) img[alt=Single_Tracking] { width: 20px; }\nIf you want to change the size, use html\n1 \u0026lt;img src=\u0026#34;{{ site.url }}/assets/images/single_object_tracking_1.png\u0026#34; alt=\u0026#34;drawing\u0026#34; width=\u0026#34;500\u0026#34;/\u0026gt; Block Quate block quate continue\nQuote break\nanther quaote that is very long long long long long long long long long long long long long long.\nReference Adapted from Markdown Cheatsheet by adam-p\nLicence: CC-BY\n","date":"2019-08-09T00:00:00Z","permalink":"http://localhost:1313/p/markdown-reference/","title":"Markdown Reference"},{"content":"This is a guide to help you setup your personal website using Github Pages and jekyll.\nWhat do you need for a personal website?\nsome webpages. a server. Webpages are some HTML that defines the content of your website.\nA server will host your website so that anybody can get access to it through the internet.\nGithub Pages allow you create a repository with the name of your accountname.github.io, if you put some HTML files on that repository and push it to github. It will generate your website for you, for free!\n1. setup Github Pages Following content will tell you how to setup Github Pages (the server side) and create a simple HTML page. The content are directly copied from Github Pages[1]\nHead over to GitHub and create a new repository named username.github.io, where username is your username (or organization name) on GitHub.\nGo to the folder where you want to store your project, and clone the new repository.\n1 git clone https://github.com/username/username.github.io Enter the project folder and add an index.html file.\n1 2 cd username.github.io echo \u0026#34;Hello World\u0026#34; \u0026gt; index.html Add, commit, and push your changes:\n1 2 3 git add --all git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin master Fire up a browser and go to https://username.github.io.\nActually, be patient as there might be a delay : )\n2. build your local website Now, you should be able to see you website hosted by github. The next step is to create your website pages and setup you blog.\nWhat do you need for your webpages?\nHTML for web content CSS for style, like layout and formatting. Jekyll is a powerful tool in a sense that you don\u0026rsquo;t need to know much about HTML or CSS for your website. It\u0026rsquo;s default theme will work fine for the style. And the cool part is that you can write webpages in Markdown. Jekyll will generate the HTML for you.\nI would recommand you go through the install guide and jekyll tutorial[1] for a step by step setup. I will list a few keypoints here for your reference.\ninstall the environment for jekyll.\ninstall Ruby \u0026gt;= 2.4 follow the guide for ubuntu.\nnote that Ubuntu user might need to add repository\n1 2 sudo apt-add-repository ppa:brightbox/ruby-ng sudo apt-get update go through the tutorial to setup your website.\nRuby 101 will give you an idea what it can do. You can follow this quick demo to get a template website with just a few lines of commands.\nAfter seeing the pattern at Ruby 101, you can start to build your own website. Go to your local clone of your website repository, follow the step by step tutorial to setup your website.\nIt major contain the following contents\ncreate a simple site adding a few pages make a navigation bar for all pages adding style adding blogs organizing blogs by author (I did this by topic) deploy your website A few comments would be.\nyou can choose some nice themes, and even change them, check the github pages for the theme for more information. for each file you generated in this repo, look for their corresponding file in Ruby 101, you may copy some of the contents (__config.yml, Gemfile) and try to understand them. 3. adding comments to your posts I think allowing interaction would be important for feedback, thus this part is added. If you definitely want to find the best solution for you, check this post for reference. A post about different methods of hosting comments.\nI pick the easiest one, using Disqus.\nHere are the steps:\ngo to register an account in Disqus. accociate your website shortname with your account, notice that this step cannot be changed once set. add templates to your website choose Jekyll as your platform. copy the Embedded code to \u0026ldquo;comments.html\u0026rdquo; in your _includes folder. You might want to uncomment some field here, check this post for reference, it use \u0026lsquo;{{ page.url absolute_url }}\u0026rsquo; for both PAGE_URL and PAGE_IDENTIFIER. add {% raw %} {% include comments.html %} {% endraw %} to your _layouts/post.html wrap the include statement with {% raw %} {% if page.comments %} \u0026hellip; {$ endif %} {% endraw %} so that you can decide if you want comments in each post by setting comments: true or comments: false. you can hide your comment by default, which make it load faster. Check out this post. add a field in your post header \u0026ldquo;comments: true\u0026rdquo; to the posts you want the comments. up to this step, your localhost should display comments correctly, but not remote. You need to edit _config.yml edit 1 url: \u0026#34;https://yourgithubname.github.io\u0026#34; add 1 2 disqus: shortname: \u0026#34;https-zhyhenryzhang-github-io\u0026#34; make sure your comments.html file include correct link 1 s.src = \u0026#39;//\u0026#39; + short_name + ... if you start with 1 s.src = \u0026#39;https://\u0026#39; + ... It might not load. a few links that you can refer to minima README.md a discussion local is able to display cause JEKYLL_ENV = development and Github Pages use production by default. So if you set local to production, you will get the same result. 4. LaTeX support Important and extremely simple! Add a few scripts to your _layout/post.html or default.html. Then you can add contents like $e=mc^2$ in \\[ \\LaTeX. \\] You can get the script from blog1 and blog2. Check MathJax documentation for reference. Note that official document mentioned that this is a limited LaTeX support.\n5. next step You have already had a nice website here, the most important thing should be blogging, to organize your knowledge and contribute to the community.\nBut there are a few things you can do also.\nupdate you github profile with you website. link your github page to the website. try out some nice themes offered by jekyll. Hacker is the one I am using, which is dark for eye care :p blogging and more. Reference: [1] https://pages.github.com\n[2] https://jekyllrb.com/docs/step-by-step/01-setup\n[3] https://github.com/pages-themes/hacker\n","date":"2019-08-08T00:00:00Z","permalink":"http://localhost:1313/p/guide-to-make-personal-website-using-github-pages-and-jekyll/","title":"Guide to make personal website using Github Pages and Jekyll"},{"content":"Notes on Multi Sensor Data Fusion from Hugh Durrant Whyte.\nThis is a lecture note referenced in the paper that I talked about in the post a multi-sensor fusion system for moving object detection and tracking in urban driving environments. In this lecture note, the probability foundations and some methods of multi sensor fusion are discussed.\n2 Probabilistic Data Fusion 2.1 Probabilistic Models Introduced definition of probability density function and its property. Conditional Probability and total probability theorem were explained. Details goes my post about probability notes\nWe use random variable $\\mathbf{x}$ to represent the state that we want to estimate. use random variable $\\mathbf{z}$ to represent the observation that we get.\n2.2 Probabilistic Methods 2.2.1 Bayes theorem Bayes theorem offers the tool to directly combine observations and prior beliefs. $$\\begin{equation} P(\\mathbf{x} | \\mathbf{z}) = \\frac{P(\\mathbf{z} | \\mathbf{x}) P(\\mathbf{x})}{P(\\mathbf{z})} . \\end{equation}$$\n2.2.2 Data Fusion using Bayes Theorem For multi sensor fusion, we want to combine the information of multiple observations, thus we have $$\\begin{equation} P(\\mathbf{x} | \\mathbf{Z^n}) = \\frac{P(\\mathbf{Z^n} | \\mathbf{x}) P(\\mathbf{x})}{P(\\mathbf{Z^n})} . \\end{equation}$$ Since it is often the case that observations are only dependent on the internal state $\\mathbf{x}$, or conditionally independent on $\\mathbf{x}$. We have $$\\begin{equation} P(\\mathbf{x} | \\mathbf{Z^n}) = CP(\\mathbf{x}) \\sum_{i=1}^{n}P(\\mathbf{z_i} | \\mathbf{x}). \\end{equation}$$\n2.2.3 Recursive Bayes Updating Do we have to start from the initial prior all the time, multiplying all the probability of observations? We can actually do it recursively.\n$$\\begin{equation} P(\\mathbf{x}|\\mathbf{Z^k}) = \\frac{P(\\mathbf{Z^k}|\\mathbf{x})P(\\mathbf{x})}{P(\\mathbf{Z^k})} \\end{equation}$$ $$\\begin{equation} = \\frac{P(\\mathbf{z_k}, \\mathbf{Z^{k-1}}|\\mathbf{x})P(\\mathbf{x})}{P(\\mathbf{Z^k})} \\end{equation}$$ $$\\begin{equation} = \\frac{P(\\mathbf{z_k} | \\mathbf{x})P(\\mathbf{Z^{k-1}}|\\mathbf{x})P(\\mathbf{x})}{P(\\mathbf{Z^k})} \\end{equation}$$ $$\\begin{equation} = \\frac{P(\\mathbf{z_k} | \\mathbf{x})P(\\mathbf{x}|\\mathbf{Z^{k-1}})P(\\mathbf{Z^{k-1}})}{P(\\mathbf{Z^k})} \\end{equation}$$ $$\\begin{equation} = \\frac{P(\\mathbf{z_k} | \\mathbf{x})P(\\mathbf{x}|\\mathbf{Z^{k-1}})}{P(\\mathbf{z_k}|\\mathbf{Z^{k-1}})}.\n\\end{equation}$$\n2.2.4 Data Dependency and Bayes Networks Short Introduction to be belief networks.\n2.2.5 Distributed Data Fusion with Bayes Theorem Distributed means raw data is processed in the sensor module. This has the advantage that each sensor is \u0026lsquo;modular\u0026rsquo; and talks in a common \u0026lsquo;state\u0026rsquo; language. However, it has the disadvantage that a complete likelihood, rather than a single observation, must be communicated.\n2.2.6 Data Fusion with Log-Likelihoods If we take log of the Beyes Theorem, it becomes $$\\begin{equation} \\mathbf{l}(\\mathbf{x} | \\mathbf{z}) = \\mathbf{l}(\\mathbf{z} | \\mathbf{x}) + \\mathbf{l}(\\mathbf{x}) - \\mathbf{l}(\\mathbf{z}). \\end{equation}$$\nThis simplifies the calculation.\n2.3 Information Measures Information is a measure of the compactness of a distribution. The shannon information (or entropy) and the Fisher information are two probabilistic measures of information of particular value in data fusion problems.\n2.3.1 Entropic Information $$\\begin{equation} H_P(\\mathbf{x}) \\triangleq - \\mathop{\\mathbb{E}} \\lbrace \\log P(\\mathbf{x})\\rbrace = - \\int_{-\\infty}^{\\infty}P(\\mathbf{x}) \\mathrm{d}\\mathbf{x}. \\end{equation}$$\nNote that $H_P(\\cdot)$ is not strictly a function of $\\mathbf{x}$ but is rather a function of the distribution $P(\\cdot)$.\n2.3.2 Conditional Entropy $$\\begin{equation} H_P(\\mathbf{x} | z_j) \\triangleq - \\mathop{\\mathbb{E}} \\lbrace \\log P(\\mathbf{x} | z_j) \\rbrace = - \\int_{-\\infty}^{\\infty}P(\\mathbf{x} | z_j) \\mathrm{d}\\mathbf{x}. \\end{equation}$$\n$$\\begin{equation} \\bar{H}(\\mathbf{x} | \\mathbf{z}) \\triangleq - \\mathop{\\mathbb{E}} \\lbrace H(\\mathbf{x} | \\mathbf{z}) \\rbrace = - \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} P(\\mathbf{x}, \\mathbf{z}) \\log P(\\mathbf{x} | \\mathbf{z}) \\mathrm{d} \\mathbf{x} \\mathrm{d}\\mathbf{z}. \\end{equation}$$\nIt shows the entropy of an n dimensional Gaussian Distribution is $$\\begin{equation} - \\frac{1}{2} \\log \\lbrack (2 \\pi e)^n | \\mathbf{P} | \\rbrack. \\end{equation}$$\nThe entropy is proportional to the log of the determinant of the covariance. The determinant of a matrix is a volume measure (recall that the determinant is the product of the eigenvalues of a matrix and the eigenvalues define axis lengths in n space).\n2.3.3 Mutual Information $$\\begin{equation} I(\\mathbf{x}, \\mathbf{z}) \\triangleq - \\mathop{\\mathbb{E}} \\lbrace \\log \\frac{P(\\mathbf{x}, \\mathbf{z})}{P(\\mathbf{x}) P(\\mathbf{z})} \\rbrace. \\end{equation}$$\n$$\\begin{equation} I(\\mathbf{x}, \\mathbf{z}) = H(\\mathbf{x}) + H(\\mathbf{z}) - H(\\mathbf{x}, \\mathbf{z}). \\end{equation}$$\nThe process of predicting information gain, making a decision on sensing action, then sensing, is an efficient and effective way of managing sensing resources and of determining optimal sensing policies.\n2.3.4 Fisher Information The Fisher information $J(\\mathbf{x})$ is defined as the second derivative of the log-likelihood $$\\begin{equation} J(\\mathbf{x}) \\triangleq \\frac{\\mathrm{d^2}}{\\mathrm{d}\\mathbf{x^2}} \\log P(\\mathbf{x}). \\end{equation}$$\nGenerally in a matrix form while cross entropy is a scaler.\n2.4 Alternatives to Probability Interval Calculus, Fuzzy logic and Evidential Reasoning is introduced shortly.\n3 Multi-Sensor Estimation 3.1 The Kalman Filter With certain assuptions about the observation and process models, the resulting estimate $\\dot{\\mathbf{x}}(t)$ minimises mean-squared error $$\\begin{equation} L(t) = \\int_{-\\infty}^{\\infty} (\\mathbf{x}(t) - \\hat{\\mathbf{x}}(t))^T (\\mathbf{x}(t) - \\hat{\\mathbf{x}}(t)) P(\\mathbf{x}(t) | \\mathbf{Z}^t) \\mathrm{d} \\mathbf{x}. \\end{equation}$$\nDifferentiation of the above equation with respect to $\\mathbf{x}(t)$ and setting equal to zero gives $$\\begin{equation} \\hat{\\mathbf{x}}(t) = \\int_{-\\infty}^{\\infty} \\mathbf{x}(t) P(\\mathbf{x}(t) | \\mathbf{Z}^t) \\mathrm{d} \\mathbf{x}. \\end{equation}$$\nwhich is simply the conditional mean $ \\hat{\\mathbf{x}}(t) = \\mathop{\\mathbb{E}}(\\mathbf{x}(t) | \\mathbf{Z}^t).$\nThe Kalman Filter, and indeed any mean-squared-error estimator, computes an estimate which is the conditional mean; an average, rather than a most likely value. (Q: what is the most likely value ?)\n3.1.1 State and Sensor Models This section defineds the notations for Kalman Filter. And shows how the model is discretized.\nThe author claimed that it is always best to start with a continuous-time model for the state and then construct a discrete model, rather than stating the discrete model at the outset. This allows for correct identification of noise transfers and noise correlations. ??\n3.1.2 The Kalman Filter Algorithm Prediction Step: $$\\begin{equation} \\hat{\\mathbf{x}}(k | k-1) = \\mathbf{F}(k) \\hat{\\mathbf{x}}(k-1 | k-1) + \\mathbf{B}(k) \\mathbf{u}(k) \\end{equation}$$ $$\\begin{equation} \\mathbf{P}(k | k-1) = \\mathbf{F}(k) \\mathbf{P}(k-1 | k-1) \\mathbf{F}^T(k) + \\mathbf{G}(k) \\mathbf{Q}(k) \\mathbf{G}^T(k) \\end{equation}$$\nUpdate Step: $$\\begin{equation} v(k) = \\mathbf{z}(k) - \\mathbf{H}(k) \\hat{\\mathbf{x}}(k | k-1) \\end{equation}$$ $$\\begin{equation} \\mathbf{S}(k) = \\mathbf{R}(k) + \\mathbf{H}(k) \\mathbf{P}(k | k-1) \\mathbf{H}^T(k) \\end{equation}$$ $$\\begin{equation} \\hat{\\mathbf{x}}(k | k) = \\hat{\\mathbf{x}}(k | k-1) - \\mathbf{W}(k) v(k) \\end{equation}$$ $$\\begin{equation} \\mathbf{P}(k | k) = \\mathbf{P}(k | k-1) - \\mathbf{W}(k) \\mathbf{S}(k) \\mathbf{W}^T(k) \\end{equation}$$ $$\\begin{equation} \\mathbf{W}(k) = \\mathbf{P}(k | k-1) \\mathbf{H}^T(k) \\mathbf{S}^{-1}(k) \\end{equation}$$\n3.1.3 The Innovation Because the \u0026rsquo;true\u0026rsquo; states are not usually available for comparison with the estimated states, the innovation is often the only measure of how well the estimator is performing. The most important property of innovation is that they form an orthogonal uncorrelated, white sequence, $$\\begin{equation} \\mathop{\\mathbb{E}} \\lbrace v(k) | \\mathbf{Z}^{k-1} \\rbrace = 0, \\mathop{\\mathbb{E}} \\lbrace v(i)v^T(jj) \\rbrace = \\mathbf{S}(i) \\delta_{ij}. \\end{equation}$$\nThis can be exploited in monitoring filter performance.\n3.1.4 Understanding the Kalman Filter There are three different, but related ways of thinking about the Kalman filter algorithm\nWeighted average Clearly, this interpretation of the Kalman filter holds only for those states which can be written as a linear combination of the observation vector.\nConditional mean\nOrthogonal decomposition\n3.1.5 Steady State Filter Using the fact that if filtering is synchronous, and state and observation matrics time-invariant, the filter variance will converge quickly, thus $\\mathbf{W}$ matrix is also converged, we can skip the computation and use paramatric model to estimate it, reduce the computation greatly.\n3.1.6 Asynchronous, Delayed and Asequent observations Asynchronous means observations from different sensors don\u0026rsquo;t come at the same time, but without delay. Can be addressed by taking time into account, make a prediction at observing time. Delayed means some observations is received by the filter with some delay after it is observed. Can be addressed by dropping the current prediction and make a prediction of the observing time of that sensor. Asequent means delay from different sensor cause the order of receiving is different from the the order of observations. It cannot be addressed easily unless you keep some observations and recompute the estimation. The author claim this can possibly be solved by a inverse-covariance filter ??.\n3.1.7 The Extended Kalman Filter Linearization at the estimation point.\nA few comments from the author:\nJacobians needs to be recomputed every time. The model is based on lieanrization at the estimated trajectory, thus if the estimation is not accurate, the second or higher order term cannot be ignored and make it useless. This also requires that initialization should be careful. 3.2 The Multi-Sensor Kalman Filter 3.2.1 Observation Models 3.2.2 The Group-Sensor Method It basiclly stacks all the observations and observation models, treating them as one sensor. The Kalman Filter can be directly applied. The problems is that sensors are assumed to be synchronous and computational complexity of matrix inversion is $\\mathbf{O}(n^3)$ as the number of sensors increases.\n3.2.3 The Sequential-Sensor Method Treat each observation and observation model independently, make a prediction and update each observation is made. The requires too much predicitons and updates when sensor number increases, though it is linear of the number of sensors. CMU paper a multi-sensor fusion system for moving object detection and tracking in urban driving environments applied this method.\n3.2.4 The Inverse-Covariance Form This is essentially a information filter, the good side is it simplifies the update step, while the only inversion would be the $P$ matrix, only of the size of states and not related to the number of sensors.\n3.2.5 Track-to-Track Fusion Sensor will do filtering locally and pass the posterior to the center, the center will fuse them together. Method can be covariance-weighted Average.\n3.3 Non-linear Data Fusion Methods Bayes filter is mentioned. Listed particle-filter and the sum-of-gaussians (gaussian mixture) method.\n3.4 Multi-Sensor Data Association The idea is to use the nomalized innovation distance.\n3.4.1 The Nearest-Neighbour Standard Filter Match the closest one, not suitable for highly clustered detections.\n3.4.2 The Probabilistic Data Association Filter Keep a mixture of all the observations within a certain range of the prediction to form a combined state and do the prediction again.\nThe good thing is you are never wrong, but you are never right as well.\n3.4.3 The Multiple Hypothesis Tracking (MHT) Filter Keep a hypothesis for all observations within certain range. Compute the likelihood and drop protential not good trackings.\n3.4.4 Data Association in Track-to-Track Fusion Weighted average.\n3.4.5 Maintaining and Managing Track Files Refer to other literature.\nOthers Section 4 metions about distributed sensor fusion and 5 metions about decision make, which are not of interest here.\nQuestions How do we discretize the kalman filter? How is $\\mathbf{HPH}^T$ a projection? How is the gain being computed? What are the three interpretation? How to use information filter to solve asequent data? How to use information filter to solve multi sensor problems? How is the steady state filter parameter being choosed? What are the difference of the association methods? Why $ \\mathbf{GQG}^T$ in kalman filter prediction? ","date":"2019-08-08T00:00:00Z","permalink":"http://localhost:1313/p/multi-sensor-data-fusion-hugh-durrant-whyte/","title":"Multi-sensor Data Fusion Hugh Durrant Whyte"},{"content":"This is the first post on this website. Thanks to Github Pages, Hugo.\n","date":"2019-08-08T00:00:00Z","permalink":"http://localhost:1313/p/posts-set/","title":"Posts Set"}]